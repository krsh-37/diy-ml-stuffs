{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ec368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229e0a0",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eaa7ff",
   "metadata": {},
   "source": [
    "Given sequence 'x' e.g. [\"hi\", \"my\", \"name\", \"is\"]\n",
    "\n",
    "we want to know the interaction (similarity) b/w each word with others.\n",
    "\n",
    "this is done by projecting 'x' via a linear layer to get q, k, v \n",
    "q -> query, what is needed\n",
    "k -> key, information about the current\n",
    "v -> value, final weight avg given similarity(calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75cd2d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9952)\n",
      "tensor(3274.3716)\n",
      "tensor(25.5810)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, s, d = 10, 4, 128\n",
    "\n",
    "x = torch.randn(b,s,d)\n",
    "print(x.var())\n",
    "similarity = x @ x.transpose(-2, -1)\n",
    "print(similarity.var())\n",
    "similarity = similarity / (d **0.5)\n",
    "print(similarity.var())\n",
    "soft = similarity.softmax(-1)\n",
    "soft[0].sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e82c6",
   "metadata": {},
   "source": [
    "## Single headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3e55c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0215, -0.1242,  0.0626,  ...,  0.1387,  0.1708,  0.0487],\n",
       "         [-0.0139, -0.1821,  0.0945,  ...,  0.1462,  0.1688,  0.0929],\n",
       "         [-0.0014, -0.1084,  0.0424,  ...,  0.0898,  0.1312,  0.0886],\n",
       "         ...,\n",
       "         [-0.0152, -0.1461,  0.0783,  ...,  0.1647,  0.1842,  0.0663],\n",
       "         [-0.0184, -0.1054,  0.0434,  ...,  0.1180,  0.1456,  0.1013],\n",
       "         [ 0.0248, -0.0857,  0.0691,  ...,  0.1254,  0.1747,  0.0700]],\n",
       "\n",
       "        [[-0.2378,  0.0009,  0.1013,  ...,  0.0725,  0.1120,  0.1676],\n",
       "         [-0.1747, -0.0497,  0.0817,  ...,  0.0491,  0.0399,  0.1744],\n",
       "         [-0.2405, -0.0036,  0.1232,  ...,  0.0151,  0.1309,  0.1887],\n",
       "         ...,\n",
       "         [-0.2745, -0.0709,  0.1324,  ...,  0.0718,  0.0901,  0.1753],\n",
       "         [-0.2727, -0.0808,  0.1612,  ...,  0.0930,  0.1021,  0.2083],\n",
       "         [-0.2279, -0.0353,  0.1329,  ...,  0.0364,  0.1422,  0.1589]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, emd_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emd_dim\n",
    "        self.q = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.k = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.v = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.q(x)\n",
    "\n",
    "        similarity = (q @ k.transpose(-2, -1)) / self.emb_dim ** 0.5\n",
    "        attention = similarity.softmax(-1)\n",
    "        output = attention @ v\n",
    "\n",
    "        return output\n",
    "    \n",
    "attn = Attention(128)\n",
    "x = torch.randn(2, 64, 128)\n",
    "attn(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909cc1c",
   "metadata": {},
   "source": [
    "## MHA encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9c485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 128])\n",
      "torch.Size([2, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionEncoded(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, attn_drop=0.1, proj_drop=0.0, bias=False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.emb_dim // n_heads\n",
    "\n",
    "        # attn\n",
    "        self.q_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        ## post attn\n",
    "        self.out_proj = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.out_proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "\n",
    "        # B -> Batch dim of input\n",
    "        # T -> Time step of sequence\n",
    "        # C -> Channel i.e embedding dim\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        ## 1. project 'x' to get q, k, v\n",
    "        ## 2. we have (B, T, C), but we need to do multi headed on the 'C'\n",
    "        ##    so, split C into (n_heads * head_dim) -> (B, T, H, C')\n",
    "        ##    But for actual calculation, we want (B H T C') @ (B H C' T) -> transpose/ swap dim (1,2)\n",
    "        q = self.q_proj(x).reshape(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        k = self.k_proj(x).reshape(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        v = self.v_proj(x).reshape(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # calc attn\n",
    "        attn = ( (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)).softmax(-1)\n",
    "        attn = self.attn_drop(attn) ## dropout some attn scores\n",
    "        # Get values\n",
    "        x = attn @ v\n",
    "\n",
    "        # swap back (B H T C) to (B T H C) and combine (H C') to C\n",
    "        x = x.transpose(1,2).reshape(B, T, C)\n",
    "\n",
    "        # Project multi-headed result to have interaction b/w heads via linear layer again \n",
    "        x = self.out_proj(x)\n",
    "        x = self.out_proj_drop(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "\n",
    "mha = MultiHeadAttentionEncoded(128, 4)\n",
    "x = torch.randn(2, 64, 128)\n",
    "mha(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9a3ef",
   "metadata": {},
   "source": [
    "## Causal & attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a993e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  True, False,  True,  True,  True,  True, False,\n",
      "          True, False, False, False,  True, False, False,  True, False, False,\n",
      "         False, False,  True,  True, False, False, False,  True,  True,  True,\n",
      "         False, False,  True, False, False, False,  True,  True,  True, False,\n",
      "         False,  True, False, False, False, False, False,  True, False, False,\n",
      "          True,  True, False,  True, False,  True,  True, False, False, False,\n",
      "         False, False,  True, False],\n",
      "        [ True,  True, False,  True, False,  True, False,  True,  True, False,\n",
      "         False, False, False,  True, False,  True, False,  True, False,  True,\n",
      "          True, False,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "          True,  True, False,  True,  True, False, False,  True, False, False,\n",
      "         False, False, False, False,  True, False, False,  True,  True, False,\n",
      "          True,  True, False, False,  True,  True, False,  True, False, False,\n",
      "         False,  True,  True, False]])\n",
      "torch.Size([2, 64, 128])\n",
      "torch.Size([2, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "class CausalMHAEncoded(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, attn_drop=0.1, proj_drop=0.0, bias=False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.emb_dim // n_heads\n",
    "\n",
    "        # attn\n",
    "        self.q_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        ## post attn\n",
    "        self.out_proj = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.out_proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        print(x.shape)\n",
    "\n",
    "        # B -> Batch dim of input\n",
    "        # T -> Time step of sequence\n",
    "        # C -> Channel i.e embedding dim\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x).reshape(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        k = self.k_proj(x).reshape(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        v = self.v_proj(x).reshape(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # calc attn\n",
    "        attn = ( (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5))\n",
    "\n",
    "        ### before doing softmax, apply the masks  \n",
    "        ### Causal mask\n",
    "        ones = torch.ones((T, T), device=attn.device)\n",
    "        causal_mask = torch.tril(ones)\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).bool() # add fake B, H dim\n",
    "        ### attn mask\n",
    "        if attn_mask is not None:\n",
    "            causal_mask = causal_mask.repeat(B, 1, 1, 1)\n",
    "            # attn_mask -> (B, T) -> add fake H, C dim -> (B 1 1 T) then repeat in C dim -> (B H T T)\n",
    "            attn_mask = attn_mask.unsqueeze(1).unsqueeze(1).repeat(1, 1, T, 1)\n",
    "\n",
    "            # add attn mask to causal mask\n",
    "            causal_mask = causal_mask.masked_fill(~attn_mask, False)\n",
    "\n",
    "        ## mask out the attn\n",
    "        attn = attn.masked_fill(~causal_mask, float(\"-inf\"))\n",
    "\n",
    "        attn = attn.softmax(-1)\n",
    "        attn = self.attn_drop(attn) ## dropout some attn scores\n",
    "        # Get values\n",
    "        x = attn @ v\n",
    "\n",
    "        # swap back (B H T C) to (B T H C) and combine (H C') to C\n",
    "        x = x.transpose(1,2).reshape(B, T, C)\n",
    "\n",
    "        # Project multi-headed result to have interaction b/w heads via linear layer again \n",
    "        x = self.out_proj(x)\n",
    "        x = self.out_proj_drop(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "\n",
    "attn_mask = torch.randint(0, 2, (2, 64)).bool()\n",
    "print(attn_mask)\n",
    "mha = CausalMHAEncoded(128, 4)\n",
    "x = torch.randn(2, 64, 128)\n",
    "mha(x, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300ed76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
