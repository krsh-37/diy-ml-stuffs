{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fb830",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c86fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da3a4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dfa21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33d9a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd644ae9",
   "metadata": {},
   "source": [
    "## KL divergence \n",
    "\n",
    "Assume we need to find the difference/ how much the probability distribution varies for the same underlying set of outcomes X.\n",
    "\n",
    "\n",
    "Lets say, we measured the probability distribution in 2 timestamp for the same set of X \n",
    "\n",
    "\n",
    "t1: P(1) = 40%, P(2) = 20%, P(3) = 40%\n",
    "t2: Q(1) = 40%, Q(2) = 40%, Q(3) = 20%\n",
    "\n",
    "for the same x from X; P -> distribution at t1, Q -> distribution at t2\n",
    "\n",
    "A simple way to calculate relative difference(P wrt Q) is to:\n",
    "\n",
    "-> P(x)/Q(x) and to get overall sense, we can average for all the x \n",
    "\n",
    "$\\frac{1}{n} \\sum_{x \\in X} \\frac{P(x)}{Q(x)}$\n",
    "\n",
    "but this is very naive way and have issues, even though P(2) and P(3) has same 20% differnce in opposite directions, the higher value skews the average.\n",
    "\n",
    "we need a function to map both to same value but in opposite direction. \n",
    "i.e if f(x) = y ; f(1/x) = -y \n",
    "\n",
    "log matches this exactly \n",
    "\n",
    "so lets fix our formula by, \n",
    "\n",
    "$\\frac{1}{n} \\sum_{x \\in X} \\log \\frac{P(x)}{Q(x)}$\n",
    "\n",
    "we have one final thing to make it better, \n",
    "\n",
    "now, we are weighing all the observations with same weightage, but the changes in higher probabilites needs to be weighted rather than smaller changes just like expectation calculation of a distribution.\n",
    "thats why we instead of equal weightage 1/n , we need to use P(x) \n",
    "\n",
    "so our final form becomes\n",
    "\n",
    "\n",
    "$\\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}$\n",
    "\n",
    "\n",
    "And that is infact our KL divergence formula for discrete function\n",
    "\n",
    "$D_{\\mathrm{KL}}(P \\| Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}$\n",
    "\n",
    "and for continuos, it is intergal over change of x\n",
    "\n",
    "$D_{\\mathrm{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} \\, dx$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c353c08",
   "metadata": {},
   "source": [
    "## KL Divergence\n",
    "\n",
    "Assume we want to find how much one probability distribution differs from another for the same underlying set of outcomes \\(X\\).\n",
    "\n",
    "Let’s say we measure the probability distribution at two time points for the same set \\(X\\):\n",
    "\n",
    "- **t1:** \\( P(1) = 40\\%, P(2) = 20\\%, P(3) = 40\\% \\)\n",
    "- **t2:** \\( Q(1) = 40\\%, Q(2) = 40\\%, Q(3) = 20\\% \\)\n",
    "\n",
    "Here, \\( P \\) is the distribution at time t1 and \\( Q \\) is the distribution at time t2.\n",
    "\n",
    "---\n",
    "\n",
    "A simple way to calculate the relative difference of \\( P \\) with respect to \\( Q \\) is to compute:\n",
    "\n",
    "\\[\n",
    "\\frac{P(x)}{Q(x)}\n",
    "\\quad \\text{and to get an overall sense, take the average:} \\quad\n",
    "\\frac{1}{n} \\sum_{x \\in X} \\frac{P(x)}{Q(x)}.\n",
    "\\]\n",
    "\n",
    "However, this is a naive approach and has issues. For example, even though \\( P(2) \\) and \\( P(3) \\) both differ by 20% (but in opposite directions), the higher value skews the average.\n",
    "\n",
    "---\n",
    "\n",
    "We need a function that maps changes in opposite directions to values with the same magnitude but opposite sign. That is, if \\( f(x) = y \\), then \\( f(1/x) = -y \\). The logarithm perfectly satisfies this:\n",
    "\n",
    "\\[\n",
    "\\log(x) = -\\log\\left(\\frac{1}{x}\\right).\n",
    "\\]\n",
    "\n",
    "So, we can improve our formula to:\n",
    "\n",
    "\\[\n",
    "\\frac{1}{n} \\sum_{x \\in X} \\log \\frac{P(x)}{Q(x)}.\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "However, this still gives equal weight to all outcomes. In reality, larger probabilities should contribute more to the measure — just like how an expectation weights each outcome by its probability.  \n",
    "\n",
    "Therefore, instead of using equal weights \\(1/n\\), we weight each term by \\( P(x) \\):\n",
    "\n",
    "\\[\n",
    "\\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}.\n",
    "\\]\n",
    "\n",
    "This is exactly the **KL divergence** formula for discrete probability distributions:\n",
    "\n",
    "\\[\n",
    "D_{\\mathrm{KL}}(P \\| Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}.\n",
    "\\]\n",
    "\n",
    "For continuous distributions, this becomes an integral:\n",
    "\n",
    "\\[\n",
    "D_{\\mathrm{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} \\, dx.\n",
    "\\]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
