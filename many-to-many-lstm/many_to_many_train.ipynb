{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88cdd75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os, re\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042f8ac",
   "metadata": {},
   "source": [
    "#### Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d60fd5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(all_files):\n",
    "    all_words = \"\"\n",
    "    for f_name in all_files:\n",
    "        all_lines = open(f_name).readlines()\n",
    "        all_lines = [ l.replace(\"\\n\", \"\")\n",
    "                     for l in all_lines if not 'Page | ' in l]\n",
    "        text = \"\".join(all_lines).lower()\n",
    "        all_words+=text\n",
    "    return all_words\n",
    "\n",
    "train_dir = \"../data/hp_book1.txt\"\n",
    "\n",
    "all_text = pre_process_text([train_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cb49a",
   "metadata": {},
   "source": [
    "#### Creating Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fe6dc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_chars = sorted(list(set(all_text)))\n",
    "\n",
    "s2i = {w: i for i, w in enumerate(uniq_chars)}\n",
    "i2s = {i: w for i, w in enumerate(uniq_chars)}\n",
    "\n",
    "encode = lambda x: [s2i[i] for i in x]\n",
    "decode = lambda x: ''.join([i2s[i] for i in x])\n",
    "\n",
    "tokenizer = s2i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ca227",
   "metadata": {},
   "source": [
    "#### Defining Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa8be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 42, 44, 27, 27, 28, 37, 35, 48,  0, 24, 37, 27,  0, 42],\n",
      "        [41, 41, 48,  0, 46, 24, 42,  0, 42, 44, 27, 27, 28, 37, 35]]) tensor([[42, 44, 27, 27, 28, 37, 35, 48,  0, 24, 37, 27,  0, 42, 32],\n",
      "        [41, 48,  0, 46, 24, 42,  0, 42, 44, 27, 27, 28, 37, 35, 48]])\n",
      " suddenly and s\n",
      "suddenly and si\n"
     ]
    }
   ],
   "source": [
    "class HPDataLoader(Dataset):\n",
    "    def __init__(self, all_text, tokenizer, max_seq_len = 200):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.text = all_text\n",
    "        self.text_len = len(all_text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return ( self.text_len - self.max_seq_len) # no. of samples possible\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # if more than seq_len, trim it\n",
    "        rand_start_idx = np.random.randint( self.__len__() )\n",
    "        sample = self.text[rand_start_idx: (rand_start_idx + self.max_seq_len) ]\n",
    "        label = self.text[(rand_start_idx+1): (rand_start_idx + 1 + self.max_seq_len) ]\n",
    "\n",
    "        ## tokenized result\n",
    "        sample = torch.tensor( [ self.tokenizer[c] for c in sample ], dtype=torch.long )\n",
    "        label = torch.tensor( [ self.tokenizer[c] for c in label ], dtype=torch.long )\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "train_ds = HPDataLoader(all_text=all_text, tokenizer=tokenizer, max_seq_len=15)\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_size=2, shuffle=True)\n",
    "\n",
    "for (s, l) in train_data_loader:\n",
    "    # (B x T x C)\n",
    "    print(s, l)\n",
    "    print(decode(s.tolist()[0]))\n",
    "    print(decode(l.tolist()[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e86e9",
   "metadata": {},
   "source": [
    "#### Defining LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (embs): Embedding(57, 8)\n",
      "  (lstm): LSTM(8, 10, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=10, out_features=57, bias=True)\n",
      ")\n",
      "hello””y55\n",
      "torch.Size([2, 15]) torch.Size([2, 15])\n",
      "torch.Size([2, 15, 57]) None\n"
     ]
    }
   ],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_size, vocab_size, n_layers):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim          # input to emb\n",
    "        self.hidden_size = hidden_size  # LSTM internal (ip -> NN) projection size\n",
    "        self.vocab_size = vocab_size    # len(tokenizer)\n",
    "        self.n_layers = n_layers        # stacked lstm layers\n",
    "\n",
    "        ## project input\n",
    "        self.embs = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.emb_dim)\n",
    "        ## define lstm\n",
    "        self.lstm = nn.LSTM(input_size=self.emb_dim, hidden_size=self.hidden_size,\n",
    "                            num_layers=self.n_layers, batch_first=True)\n",
    "        ## out classifier\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        embs = self.embs(x)                     # B x T x emb_dim\n",
    "        ## pass through lstm\n",
    "        outputs, (hn, cn) = self.lstm(embs)     # B x T x hddn_size\n",
    "        # Take last out and return out class\n",
    "        outputs = self.dropout(outputs)         # B x T x hddn_size\n",
    "        logits = self.fc(outputs)               # B x T x vocab_size\n",
    "\n",
    "        ## cross entropy loss\n",
    "        loss = None\n",
    "        if not targets is None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, text, max_new_tokens=200, greedy=False): \n",
    "        with torch.no_grad():\n",
    "            ip_tokens = torch.tensor( [tokenizer[c] for c in text] ).unsqueeze(0) # add fake batch dim\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits = self.forward(ip_tokens)[0]           # B x T x vocab_size\n",
    "                logits = logits[: , -1, :]                    # take last time step\n",
    "                probs = F.softmax(logits, dim=-1)             # convert logits to probs\n",
    "                if greedy:\n",
    "                    next_idx = torch.multinomial(probs, num_samples=1)\n",
    "                else:\n",
    "                    next_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "                ip_tokens = torch.cat( (ip_tokens, next_idx), dim=1)\n",
    "            return f\"{decode(ip_tokens.tolist()[0])}\"\n",
    "    \n",
    "model = LSTMNet(emb_dim=8, hidden_size=10, vocab_size=len(tokenizer), n_layers=2)\n",
    "print(model)\n",
    "for x, target in train_data_loader:\n",
    "    print(model.generate(\"hello\", 5, greedy=False))\n",
    "    \n",
    "    print(x.shape, target.shape)\n",
    "    outs, loss = model(x)\n",
    "    print(outs.shape, loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39c316",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe5381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "model = LSTMNet(\n",
      "  (embs): Embedding(57, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=57, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## defining pre-train\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "batch_size = 128\n",
    "\n",
    "## model\n",
    "model = LSTMNet(\n",
    "    emb_dim=128,\n",
    "    hidden_size=256,\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_layers=2\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "print(f\"{model = }\")\n",
    "\n",
    "optm = optim.Adam(params=model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "26ec5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, train_data_loader, optm, max_counter = 3000):\n",
    "    logs = {\n",
    "        \"epoch\" : [],\n",
    "        \"training_loss\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"Epoch: {epoch}/ {epochs}\")\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        counter, max_counter = 0, max_counter\n",
    "        for x, targets in tqdm(train_data_loader, desc = \"Training\"):\n",
    "            ## data through model\n",
    "            x, targets = x.to(DEVICE), targets.to(DEVICE)\n",
    "            optm.zero_grad()\n",
    "            outputs, loss = model(x, targets)\n",
    "\n",
    "            ## Loss \n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            ## clip exp gradiants\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optm.step()\n",
    "\n",
    "            counter += 1\n",
    "            if counter >= max_counter:\n",
    "                break\n",
    "\n",
    "        print(\"=\"*50)\n",
    "        print(\":: Generating text : non-greedy\")\n",
    "        print( model.generate(\"harry\", 100) )\n",
    "        print(\"=\"*50)\n",
    "        print(\":: Generating text : greedy\")\n",
    "        print( model.generate(\"harry\", 100) )\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        logs[\"epoch\"].append(epoch)\n",
    "        logs[\"training_loss\"].append(np.mean(train_loss))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch} | \"\n",
    "            f\"Train Loss: {logs['training_loss'][-1]:.4f} \"\n",
    "        )\n",
    "\n",
    "    return logs, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "236b11f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 2999/218185 [00:48<57:32, 62.33it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ":: Generating text : non-greedy\n",
      "harry was a long to the stone and harry was to the stone was had a started and him and harry was the ston\n",
      "==================================================\n",
      ":: Generating text : greedy\n",
      "harry was had had harry was had a stone was a stone was the started to the stone was to the stand the sto\n",
      "==================================================\n",
      "Epoch 1 | Train Loss: 1.7340 \n",
      "Epoch: 2/ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 2999/218185 [00:47<56:54, 63.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ":: Generating text : non-greedy\n",
      "harry was a started to his had been his for the bottle and his had a beard the standing the stands and he\n",
      "==================================================\n",
      ":: Generating text : greedy\n",
      "harry was a because and the couldn’t had been his had been his had been his been the started the place an\n",
      "==================================================\n",
      "Epoch 2 | Train Loss: 1.6490 \n"
     ]
    }
   ],
   "source": [
    "train_logs, model = train(\n",
    "    epochs=2,\n",
    "    model=model,\n",
    "    train_data_loader=train_data_loader,\n",
    "    optm=optm\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), \"final_model___.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4726687d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harry way magit. he couldn she said. “you lunt and chut who was posting. whated however. he points the wa'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = LSTMNet(\n",
    "    emb_dim=128,\n",
    "    hidden_size=256,\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_layers=2\n",
    ")\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(\"final_model.pt\", map_location=torch.device(DEVICE)))\n",
    "loaded_model.eval()\n",
    "loaded_model.generate(\"harry\", 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "38c9add8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harry couldn’t instice out in that, but is a durslaching familitted to get his imbinish the few yer antendstre.” “a fel a nosten it iustence. “wa painst be arstmort, brush the lowed through a baxid are lea'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.generate(\"harry\", 200, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
