{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcd1dbc",
   "metadata": {},
   "source": [
    "### Data shapes Flow \n",
    "\n",
    "```py\n",
    "input: (B, T)\n",
    "\n",
    "C -> emb dim\n",
    "\n",
    "1. emb: (input) -> (B, T, C) : emb + pos_emb\n",
    "2. Block:\n",
    "    MHA: (B, T, C) -> (B, T, H, C') -> (B, H, T, C') -> mha -> (B T H C') -> (B T C) \n",
    "    Feedfwd: (B T C) -> C -> expands in hidden dim then comes to -> (B T C)\n",
    "\n",
    "3. Linear: (B T C) -> (B T vocab_size)\n",
    "4. calc Loss fn: \n",
    "    calc loss b/w pred (B * T, vocab) vs target (B * T,)\n",
    "    -> (B T vocab) -> (B * T, vocab) -> softmax -> [ (BT, vocab), (BT, ) ] -> (BT, -log(p_true_class) ) -> mean(BT, 1) -> single scaler loss and backprob\n",
    "        loss = -prob[np.arange(n), ys].log().mean()\n",
    "```\n",
    "e.g.\n",
    "```py\n",
    "    logits_flat =\n",
    "    [[2.0, 1.0, 0.1],    # target: 0\n",
    "    [1.0, 3.0, 0.2],    # target: 1\n",
    "    [0.5, 2.0, 0.3],    # target: 1\n",
    "    [1.5, 0.2, 2.0]]    # target: 2\n",
    "\n",
    "    targets_flat = [0, 1, 1, 2]\n",
    "\n",
    "    loss = log_softmax(logits)  # then picks the log-prob for the correct class\n",
    "    loss = -log(p_true_class)\n",
    "    loss = mean over all examples\n",
    "\n",
    "    Input: [2.0, 1.0, 0.1]\n",
    "    Softmax = exp(x_i) / sum(exp(x_j))\n",
    "            = [e^2, e^1, e^0.1] / sum(...)\n",
    "            ≈ [7.389, 2.718, 1.105] / (7.389+2.718+1.105)\n",
    "            ≈ [0.659, 0.242, 0.098]\n",
    "\n",
    "    Log-softmax ≈ [log(0.659), log(0.242), log(0.098)] \n",
    "                ≈ [-0.417, -1.417, -2.325]\n",
    "\n",
    "    Loss_1 = -log(0.659) ≈ 0.417\n",
    "```\n",
    "| Row | Logits           | Softmax probs          | Target | Loss                |\n",
    "| --- | ---------------- | ---------------------- | ------ | ------------------- |\n",
    "| 1   | \\[2.0, 1.0, 0.1] | \\[0.659, 0.242, 0.098] | 0      | -log(0.659) ≈ 0.417 |\n",
    "| 2   | \\[1.0, 3.0, 0.2] | \\[0.113, 0.836, 0.051] | 1      | -log(0.836) ≈ 0.179 |\n",
    "| 3   | \\[0.5, 2.0, 0.3] | \\[0.211, 0.627, 0.162] | 1      | -log(0.627) ≈ 0.466 |\n",
    "| 4   | \\[1.5, 0.2, 2.0] | \\[0.331, 0.097, 0.571] | 2      | -log(0.571) ≈ 0.560 |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
