{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88cdd75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, re\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/pranayprasad/aclimdb?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111M/111M [00:11<00:00, 10.1MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"pranayprasad/aclimdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e1aabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d60fd5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(all_files):\n",
    "    all_words, seq_len = [], []\n",
    "    for f_name in all_files:\n",
    "        text = open(f_name).readlines()[0].lower()\n",
    "        text = re.sub ( r'[^\\w\\s]', '', text)\n",
    "        words = text.split(\" \")\n",
    "        words = [w for w in words if (w not in stopwords) and (len(w) >=0) ]\n",
    "        all_words+=words\n",
    "        seq_len.append(len(words))\n",
    "    return (all_words, seq_len)\n",
    "\n",
    "train_dir = \"../../data/aclImdb/train\"\n",
    "\n",
    "all_files = ([ os.path.join ( train_dir,  f\"pos/{f_name}\") for f_name in  os.listdir(f\"{train_dir}/pos\")] + \n",
    "             [ os.path.join ( train_dir,  f\"neg/{f_name}\") for f_name in  os.listdir(f\"{train_dir}/neg\")] )\n",
    "\n",
    "train_words, sentence_len = pre_process_text(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2cbffa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'avg sentence length: 125.16576'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"avg sentence length: {np.mean(sentence_len)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cb49a",
   "metadata": {},
   "source": [
    "#### Creating Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6dc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "bog = dict(Counter(train_words))\n",
    "words = sorted([key for (key,value) in bog.items() if value > 500])\n",
    "\n",
    "words.append(\"<UNK>\")\n",
    "words.append(\"<PAD>\")\n",
    "\n",
    "w2i = {w: i for i, w in enumerate(words)}\n",
    "i2w = {i: w for i, w in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa8be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[990, 371, 664,  ..., 991, 991, 991],\n",
      "        [990, 723, 990,  ..., 991, 991, 991],\n",
      "        [990, 600, 314,  ..., 991, 991, 991],\n",
      "        ...,\n",
      "        [321, 682, 886,  ...,  87, 480, 510],\n",
      "        [516, 369, 562,  ..., 991, 991, 991],\n",
      "        [306, 314, 990,  ..., 991, 991, 991]]) tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "class IMDBDataLoader(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_seq_len = 200):\n",
    "        self.tokenizer = w2i\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data_files = ([ os.path.join ( data_path,  f\"pos/{f_name}\") for f_name in  os.listdir(f\"{data_path}/pos\")] + \n",
    "             [ os.path.join ( data_path,  f\"neg/{f_name}\") for f_name in  os.listdir(f\"{data_path}/neg\")] )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data_files[idx]\n",
    "\n",
    "        def get_sample(f_name):\n",
    "            text = open(f_name).readlines()[0].lower()\n",
    "            text = re.sub ( r'[^\\w\\s]', '', text)\n",
    "            words = text.split(\" \")\n",
    "            return [w for w in words if (w not in stopwords) and (len(w) >=0) ]\n",
    "\n",
    "        def get_tokenzied_sample_seq(f_name):\n",
    "            sample = get_sample(f_name)\n",
    "            # if more than seq_len, trim it\n",
    "            if len(sample) > self.max_seq_len:\n",
    "                rand_start_idx = np.random.randint( (len(sample) - self.max_seq_len) )\n",
    "                sample = sample[rand_start_idx: (rand_start_idx + self.max_seq_len) ]\n",
    "\n",
    "            ## tokenized result\n",
    "            tokenized = []\n",
    "            for w in sample:\n",
    "                if w in self.tokenizer:\n",
    "                    tokenized.append(self.tokenizer[w])\n",
    "                else:\n",
    "                    tokenized.append(self.tokenizer[\"<UNK>\"])\n",
    "            \n",
    "            sample = torch.tensor(tokenized)\n",
    "            return sample\n",
    "\n",
    "        sample = get_tokenzied_sample_seq(file_path)\n",
    "\n",
    "        ## label\n",
    "        label = 1\n",
    "        if \"neg\" in file_path: label = 0\n",
    "        return sample, label\n",
    "\n",
    "def data_collator(batch):\n",
    "    word_tokens, labels = [], []\n",
    "    for token, label in batch:\n",
    "        labels.append(label)\n",
    "        word_tokens.append(token)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    word_tokens = nn.utils.rnn.pad_sequence(word_tokens, batch_first=True, padding_value=w2i[\"<PAD>\"])\n",
    "    return word_tokens, labels\n",
    "\n",
    "train_ds = IMDBDataLoader(train_dir, tokenizer=w2i)\n",
    "\n",
    "data_loader = DataLoader(dataset=train_ds, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "for (s, l) in data_loader:\n",
    "    # (B x T x C)\n",
    "    print(s, l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968c3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
